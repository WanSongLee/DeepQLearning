{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import time\n",
    "from utils.prioritized_memory import Memory as ReplayMemory\n",
    "import datetime\n",
    "\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using gpu\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(observation_size, 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512 , 256),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(256 , 128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(128 , 64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(64 , 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32 , 16),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(16 , 8),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(8, action_size))\n",
    "\n",
    "    def forward(self, observation):\n",
    "        return self.fc(observation)\n",
    "        \n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        observation_size = 128\n",
    "        action_size = 4\n",
    "        \n",
    "        self.q_network = DQN(observation_size, action_size).to(device)\n",
    "        self.target_network = DQN(observation_size, action_size).to(device)\n",
    "        \n",
    "        # copy the weights from q_network to target_network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "    # act greedily with respect to the Q-function\n",
    "    def act(self, state):\n",
    "        # state: 1d numpy arrray\n",
    "        out = self.q_network(state)\n",
    "        out = torch.max(out, 0)[1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, env, agent, memory, learning_rate, batch_size, target_update_freq,\n",
    "                 epsilon_start, epsilon_min, epsilon_decay_rate, epsilon_test):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.memory = memory\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.epsilon_test = epsilon_test\n",
    "        \n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "        self.optimizer = optim.Adam(agent.q_network.parameters(), learning_rate)\n",
    "        \n",
    "        # counter used to track when we need to update our target network\n",
    "        self.update_counter = 0\n",
    "        self.episode_counter = 0\n",
    "        \n",
    "        self.loss_history = []\n",
    "        self.avg_loss_history = []\n",
    "        self.frame_count = 0\n",
    "    \n",
    "    # add items to the prioritized replay memory\n",
    "    def append_sample(self, state, action, next_state, reward, done):\n",
    "        current_q = self.agent.q_network(state.to(device))[action]\n",
    "        \n",
    "        ###########single dqn#############\n",
    "#         max_next_q = self.agent.target_network(next_state.to(device)).max()\n",
    "#         target_q = reward + (1-done) * (GAMMA * max_next_q)\n",
    "        ###########single dqn#############\n",
    "\n",
    "        ###########double dqn#############\n",
    "        best_next_action = self.agent.q_network(next_state.to(device)).max(0)[1]\n",
    "        best_next_q = self.agent.target_network(next_state.to(device))[best_next_action]\n",
    "        target_q = reward + (1-done) * (GAMMA * best_next_q)\n",
    "        ###########double dqn#############\n",
    "        \n",
    "        # calculate and save the error to our memory, use for importance sampling\n",
    "        error = abs((current_q - target_q).item())\n",
    "        self.memory.add(error, (state.numpy(), action, reward, next_state.numpy(), done))\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.tree.n_entries < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_freq == 0:\n",
    "            self.agent.target_network.load_state_dict(self.agent.q_network.state_dict())\n",
    "        \n",
    "        # get prioritized experience from memory \n",
    "        mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
    "        mini_batch = np.array(mini_batch).transpose()\n",
    "        \n",
    "        states = mini_batch[0].tolist()\n",
    "        actions = mini_batch[1].tolist()\n",
    "        rewards = mini_batch[2].tolist()\n",
    "        next_states = mini_batch[3].tolist()\n",
    "        dones = mini_batch[4].tolist()\n",
    "        \n",
    "        # convert from numpy ndarray to pytorch tensor\n",
    "        states = torch.from_numpy(np.stack(states)).to(device)\n",
    "        actions = torch.IntTensor(actions).to(device)\n",
    "        next_states = torch.from_numpy(np.stack(next_states)).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "        current_q = self.agent.q_network(states).gather(1, actions.unsqueeze(1).long()).squeeze(1)\n",
    "        \n",
    "        ###########single dqn#############\n",
    "#         max_next_q = self.agent.target_network(next_states).detach().max(1)[0]\n",
    "#         target_q = rewards + (1-dones) * (GAMMA * max_next_q)\n",
    "        ###########single dqn#############\n",
    "        \n",
    "        ###########double dqn#############\n",
    "        best_next_actions = self.agent.q_network(next_states).detach().max(1)[1]\n",
    "        best_next_q = self.agent.target_network(next_states).gather(1, best_next_actions.unsqueeze(1)).squeeze(1).detach()\n",
    "        target_q = rewards + (1-dones) * (GAMMA * best_next_q)\n",
    "        ###########double dqn#############\n",
    "        \n",
    "        # update error in memory\n",
    "        errors = torch.abs(current_q - target_q).cpu().data.numpy()\n",
    "        for i in range(self.batch_size):\n",
    "            idx = idxs[i]\n",
    "            self.memory.update(idx, errors[i])\n",
    "            \n",
    "        # each transition has its own importance-sampling weight\n",
    "        temp = F.smooth_l1_loss(current_q, target_q, reduction ='none')\n",
    "        loss = (torch.FloatTensor(is_weights).to(device) * temp).mean()\n",
    "        \n",
    "        self.loss_history.append(loss.item())\n",
    "        avg_loss = self.loss_history[-1000:]\n",
    "        self.avg_loss_history.append(sum(avg_loss)/len(avg_loss))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # complete an episode, for each transitions, add it to the replay memory, then learn from the memory\n",
    "    def play_episode(self, test=False):\n",
    "        self.episode_counter += 1\n",
    "        state = self.env.reset()\n",
    "        state = torch.from_numpy(state).float()\n",
    "\n",
    "        if render:\n",
    "            self.env.render()\n",
    "\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            if test:\n",
    "                epsilon = self.epsilon_test\n",
    "            else:\n",
    "                epsilon = max(1 - self.epsilon_decay_rate * self.update_counter, self.epsilon_min)\n",
    "\n",
    "            # select action using epsilon-greedy policy\n",
    "            if random.random() < epsilon: # explore\n",
    "                action = self.env.action_space.sample()\n",
    "            else: # exploit\n",
    "                action = self.agent.act(state.to(device)).item()\n",
    "              \n",
    "            # send our action to the environment\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = torch.from_numpy(next_state).float()\n",
    "\n",
    "            score += reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "                \n",
    "            # save our experience to the memory\n",
    "            self.append_sample(state, action, next_state, reward, done)\n",
    "            \n",
    "            if not test:\n",
    "                self.learn()\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(PATH, trainer, train_scores, avg_train_scores,\n",
    "                    test_scores, avg_test_scores, total_time):\n",
    "    agent = trainer.agent\n",
    "    memory = trainer.memory\n",
    "    sumtree = memory.tree\n",
    "    \n",
    "    torch.save({\n",
    "                'memory_e': memory.e,\n",
    "                'memory_a': memory.a,\n",
    "                'memory_beta': memory.beta,\n",
    "                'memory_capacity': memory.capacity,\n",
    "                'sumtree_tree': sumtree.tree,\n",
    "                'sumtree_data': sumtree.data,\n",
    "                'sumtree_nentries': sumtree.n_entries,\n",
    "                \n",
    "                'agent_qnetwork': agent.q_network.state_dict(),\n",
    "                'agent_targetnetwork': agent.target_network.state_dict(),\n",
    "        \n",
    "                'trainer_learnrate': trainer.learning_rate,\n",
    "                'trainer_batchsize': trainer.batch_size,\n",
    "                'trainer_updatefreq': trainer.target_update_freq,\n",
    "                'trainer_epsilonstart': trainer.epsilon_start,\n",
    "                'trainer_epsilonmin': trainer.epsilon_min,\n",
    "                'trainer_epsilon_decay_rate': trainer.epsilon_decay_rate,\n",
    "                'trainer_epsilon_test': trainer.epsilon_test,\n",
    "                \n",
    "                'trainer_optimizer': trainer.optimizer.state_dict(),\n",
    "                'trainer_losshistory': trainer.loss_history,\n",
    "                'trainer_avglosshistory': trainer.avg_loss_history,\n",
    "                'trainer_episodecounter': trainer.episode_counter,\n",
    "                'trainer_updatecounter': trainer.update_counter,\n",
    "                \n",
    "                'trainscores': train_scores,\n",
    "                'avgtrainscores': avg_train_scores,\n",
    "                'testscores': test_scores,\n",
    "                'avgtestscores': avg_test_scores,\n",
    "                'totaltime': total_time\n",
    "                }, PATH)\n",
    "\n",
    "def load_checkpoint(PATH, env):\n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    memory = ReplayMemory(checkpoint['memory_capacity'])\n",
    "    memory.e = checkpoint['memory_e']\n",
    "    memory.a = checkpoint['memory_a']\n",
    "    memory.beta = checkpoint['memory_beta']\n",
    "    memory.tree.tree = checkpoint['sumtree_tree']\n",
    "    memory.tree.data = checkpoint['sumtree_data']\n",
    "    memory.tree.n_entries = checkpoint['sumtree_nentries']\n",
    "\n",
    "    agent = Agent()\n",
    "    agent.q_network.load_state_dict(checkpoint['agent_qnetwork'])\n",
    "    agent.target_network.load_state_dict(checkpoint['agent_targetnetwork'])\n",
    "    \n",
    "    trainer = Trainer(env, agent, memory, \n",
    "                     checkpoint['trainer_learnrate'], \n",
    "                     checkpoint['trainer_batchsize'], \n",
    "                     checkpoint['trainer_updatefreq'], \n",
    "                     checkpoint['trainer_epsilonstart'], \n",
    "                     checkpoint['trainer_epsilonmin'], \n",
    "                     checkpoint['trainer_epsilon_decay_rate'],\n",
    "                     checkpoint['trainer_epsilon_test'])\n",
    "    trainer.optimizer.load_state_dict(checkpoint['trainer_optimizer'])\n",
    "    trainer.loss_history = checkpoint['trainer_losshistory']\n",
    "    trainer.avg_loss_history = checkpoint['trainer_avglosshistory']\n",
    "    trainer.episode_counter = checkpoint['trainer_episodecounter']\n",
    "    trainer.update_counter = checkpoint['trainer_updatecounter']\n",
    "        \n",
    "    train_scores = checkpoint['trainscores']\n",
    "    avg_train_scores = checkpoint['avgtrainscores'] \n",
    "    test_scores = checkpoint['testscores']\n",
    "    avg_test_scores = checkpoint['avgtestscores']\n",
    "    total_time = checkpoint['totaltime']\n",
    "    \n",
    "    print(\"learning rate\", trainer.learning_rate)\n",
    "    print(\"batch_size\", trainer.batch_size)\n",
    "    print(\"target_update_freq\", trainer.target_update_freq)\n",
    "    print(\"epsilon_start\", trainer.epsilon_start)\n",
    "    print(\"epsilon_min\", trainer.epsilon_min)\n",
    "    print(\"epsilon_decay_rate\", trainer.epsilon_decay_rate)\n",
    "    print(\"memory capacity\", memory.capacity)\n",
    "    \n",
    "    print(agent.q_network)\n",
    "    print(\"number of parameters:\" ,sum([p.numel() for p in agent.q_network.parameters()]))\n",
    "    \n",
    "    return trainer, train_scores, avg_train_scores, test_scores, avg_test_scores, total_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-ram-v0')\n",
    "record = True\n",
    "if record:\n",
    "    struct = time.localtime(time.time())\n",
    "    video_path = './videos/BreakoutRam/'\n",
    "    video_path += \"{0}-{1} {2}-{3}-{4}\".format(struct.tm_mon,struct.tm_mday,struct.tm_hour,struct.tm_min,struct.tm_sec)\n",
    "    print('saving recorded videos to path \"{0}\"'.format(video_path))\n",
    "    env = wrappers.Monitor(env, video_path, video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "PATH = 'checkpoint/BreakoutRam.checkpoint'\n",
    "load = False\n",
    "\n",
    "if load:\n",
    "    trainer, train_scores, avg_train_scores, test_scores, avg_test_scores, total_time = load_checkpoint(PATH, env)\n",
    "    env.episode_id = trainer.episode_counter\n",
    "else: \n",
    "    train_scores, avg_train_scores, test_scores, avg_test_scores = [], [], [], []\n",
    "    total_time = 0\n",
    "    \n",
    "    learning_rate = 0.00001\n",
    "    batch_size = 32\n",
    "    target_update_freq = 10000\n",
    "    epsilon_start = 1\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay_rate = 0.0000009\n",
    "    epsilon_test = 0.0005\n",
    "\n",
    "    memory = ReplayMemory(1000000)\n",
    "    agent = Agent()\n",
    "    trainer = Trainer(env, agent, memory, learning_rate, batch_size, target_update_freq,\n",
    "                     epsilon_start, epsilon_min, epsilon_decay_rate, epsilon_test)\n",
    "    print(agent.q_network)\n",
    "    print(\"number of parameters:\" ,sum([p.numel() for p in agent.q_network.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "render = False\n",
    "    \n",
    "test_every = 20\n",
    "save_every = 200\n",
    "\n",
    "fig = plt.figure()\n",
    "train_scores_ax = fig.add_subplot(311)\n",
    "test_scores_ax = fig.add_subplot(312)\n",
    "loss_ax = fig.add_subplot(313)\n",
    "plt.ion()\n",
    "\n",
    "previous_time = time.time()\n",
    "for i in range(100000):\n",
    "    if trainer.episode_counter % test_every == 0:\n",
    "        score = trainer.play_episode(test=True)\n",
    "        test_scores.append(score)\n",
    "        \n",
    "        avg_score = test_scores[-100:]\n",
    "        avg_test_scores.append(sum(avg_score)/len(avg_score))\n",
    "        \n",
    "        print(\"Episode {0} finished with score of {1}\"\n",
    "                  .format(trainer.episode_counter, score))\n",
    "    else:\n",
    "        score = trainer.play_episode(test=False)\n",
    "        train_scores.append(score)\n",
    "\n",
    "        avg_score = train_scores[-100:]\n",
    "        avg_train_scores.append(sum(avg_score)/len(avg_score))\n",
    "        print(\"Episode {0}({1}) finished with score of {2}, after learning from {3} frames\"\n",
    "                  .format(trainer.episode_counter, trainer.episode_counter - len(test_scores), score, trainer.update_counter))\n",
    "    \n",
    "    train_scores_ax.clear()\n",
    "    train_scores_ax.plot(train_scores)\n",
    "    train_scores_ax.plot(avg_train_scores)\n",
    "    \n",
    "    test_scores_ax.clear()\n",
    "    test_scores_ax.plot(test_scores)\n",
    "    test_scores_ax.plot(avg_test_scores)                         \n",
    "    \n",
    "    loss_ax.clear()\n",
    "    loss_ax.semilogy(trainer.loss_history)\n",
    "    loss_ax.semilogy(trainer.avg_loss_history)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    if trainer.episode_counter % save_every == 0 and i > 0:\n",
    "        current_time = time.time()\n",
    "        total_time += current_time - previous_time\n",
    "        save_checkpoint(PATH, trainer, train_scores, avg_train_scores,\n",
    "                        test_scores, avg_test_scores, total_time)\n",
    "        print(\"took {0} seconds to run the last 200 episodes\".format(current_time - previous_time, save_every))\n",
    "        print(\"took {0} to run everything\".format(datetime.timedelta(seconds = int(total_time))))\n",
    "        previous_time = current_time\n",
    "        \n",
    "    \n",
    "trainer.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show result of training\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "print(\"loading\")\n",
    "PATH = 'checkpoint/BreakoutRam.checkpoint'\n",
    "checkpoint = torch.load(PATH)\n",
    "\n",
    "print('memory capacity: ', checkpoint['memory_capacity'])\n",
    "print('learning rate:', checkpoint['trainer_learnrate']) \n",
    "print('batch size:', checkpoint['trainer_batchsize']) \n",
    "print('target network update frequency:', checkpoint['trainer_updatefreq']) \n",
    "print('epsilon start:', checkpoint['trainer_epsilonstart']) \n",
    "print('epsilon minimum:', checkpoint['trainer_epsilonmin'])\n",
    "print('epsilon decay rate:', checkpoint['trainer_epsilon_decay_rate'])\n",
    "print('total time:', datetime.timedelta(seconds = int(checkpoint['totaltime'])))\n",
    "\n",
    "train_scores = checkpoint['trainscores']\n",
    "avg_train_scores = checkpoint['avgtrainscores'] \n",
    "test_scores = checkpoint['testscores']\n",
    "avg_test_scores = checkpoint['avgtestscores']\n",
    "\n",
    "plt.title(\"training scores\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(train_scores)\n",
    "plt.plot(avg_train_scores)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"testing scores\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(test_scores)\n",
    "plt.plot(avg_test_scores)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"loss\")\n",
    "plt.semilogy(checkpoint['trainer_losshistory'])\n",
    "plt.semilogy(checkpoint['trainer_avglosshistory'])\n",
    "# plt.semilogy(checkpoint['trainer_lastxlosshistory'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
